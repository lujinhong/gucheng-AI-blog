{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的灵活性也是它们的主要缺点之一：有许多需要调整的超参数。你不仅可以使用任何可以想象的网络结构，而且即使在简单的MLP中，你也可以更改层数、每层神经元数、每层要使用的激活函数的类型、权重初始化逻辑，以及更多。\n",
    "\n",
    "一种选择是简单地尝试超参数的许多组合，然后查看哪种对验证集最有效（或使用K折交叉验证）。例如我们可以像第2章中一样使用GridSearchCV或RandomizedSearchCV来探索超参数空间。为此我们需要将Keras模型包装在模仿常规ScikitLearn回归器的对象中。\n",
    "\n",
    "下面我们详细介绍在tensorflow中进行超参数搜索的方式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib as mpl\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们先构建一个基本模型用于之后的超参数搜索。本次我们使用housing数据集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(X_train)\n",
    "x_valid = scaler.transform(X_valid)\n",
    "x_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "超参数搜索，一种选择是简单地尝试超参数的许多组合，然后查看哪种对验证集最有效（或使用K折交叉验证）。例如我们可以使用GridSearchCV或RandomizedSearchCV来探索超参数空间。为此我们需要将Keras模型包装在模仿常规ScikitLearn回归器的对象中。第一步是创建一个函数，该函数将在给定一组超参数的情况下构建并编译Keras模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=10, learning_rate=0.0001, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics='mse')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们简单看一下只运行一次模型的情况。\n",
    "\n",
    "指定任何超参数，因此它将使用我们在build_model（）中定义的默认超参数。现在，我们可以像常规ScikitLearn回归器一样使用该对象：我们可以使用其fit（）方法进行训练，然后使用其score（）方法进行评估，然后使用predict()方法预测。\n",
    "\n",
    "传递给fit（）方法的任何其他参数都将传递给内部的Keras模型。还要注意，该分数将与MSE相反，因为ScikitLearn希望获得分数，而不是损失（即分数越高越好）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 5.3889 - mse: 5.3889 - val_loss: 33.7463 - val_mse: 33.7463\n",
      "Epoch 2/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 4.4376 - mse: 4.4376 - val_loss: 29.8043 - val_mse: 29.8043\n",
      "Epoch 3/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 4.9137 - mse: 4.9137 - val_loss: 26.2964 - val_mse: 26.2964\n",
      "Epoch 4/5\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 3.4705 - mse: 3.4705 - val_loss: 22.9076 - val_mse: 22.9076\n",
      "Epoch 5/5\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.8230 - mse: 2.8230 - val_loss: 19.6532 - val_mse: 19.6532\n",
      "162/162 [==============================] - 0s 866us/step - loss: 2.5638 - mse: 2.5638\n",
      "-2.56375789642334\n",
      "[1.3969504  0.6476435  0.48683828 1.5239983  0.50394875]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n",
    "keras_reg.fit(x_train, y_train, epochs=5,\n",
    "             validation_data=(x_valid, y_valid),\n",
    "             callbacks=[keras.callbacks.EarlyStopping(patience=5)])\n",
    "mse_test = keras_reg.score(x_test, y_test)\n",
    "y_pred = keras_reg.predict(x_test[:5])\n",
    "print(mse_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们开始超参数搜索。\n",
    "\n",
    "我们不想训练和评估这样的单个模型，尽管我们想训练数百个变体，并查看哪种变体在验证集上表现最佳。由于存在许多超参数，因此最好使用随机搜索而不是网格搜索。让我们尝试探索隐藏层的数量、神经元的数量和学习率：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)\n",
    "rnd_search_cv.fit(x_train, x_train, epochs=10,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
