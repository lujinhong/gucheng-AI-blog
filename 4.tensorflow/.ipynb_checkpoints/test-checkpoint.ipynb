{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c087d59-f521-4636-b7dc-bffc3a508665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets: (60000, 28, 28) (60000, 28, 28) 0 255\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 28, 28, 6)         156       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_14 (Averag (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 10, 10, 16)        2416      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_15 (Averag (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 1, 1, 120)         48120     \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "60000/60000 - mean: 0.0120 - mean_absolute_error: 0.0250 - categorical_accuracy: 0.9086\n",
      "60000/60000 - mean: 0.0120 - mean_absolute_error: 0.0250 - categorical_accuracy: 0.9086\n",
      "Epoch 2/5\n",
      "60000/60000 - mean: 0.0032 - mean_absolute_error: 0.0059 - categorical_accuracy: 0.9798\n",
      "60000/60000 - mean: 0.0032 - mean_absolute_error: 0.0059 - categorical_accuracy: 0.9798\n",
      "Epoch 3/5\n",
      "60000/60000 - mean: 0.0027 - mean_absolute_error: 0.0043 - categorical_accuracy: 0.9835\n",
      "60000/60000 - mean: 0.0027 - mean_absolute_error: 0.0043 - categorical_accuracy: 0.9835\n",
      "Epoch 4/5\n",
      "60000/60000 - mean: 0.0024 - mean_absolute_error: 0.0036 - categorical_accuracy: 0.9854\n",
      "60000/60000 - mean: 0.0024 - mean_absolute_error: 0.0036 - categorical_accuracy: 0.9854\n",
      "Epoch 5/5\n",
      "60000/60000 - mean: 0.0022 - mean_absolute_error: 0.0032 - categorical_accuracy: 0.9867\n",
      "60000/60000 - mean: 0.0022 - mean_absolute_error: 0.0032 - categorical_accuracy: 0.9867\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics, losses\n",
    "\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = datasets.mnist.load_data()\n",
    "print('datasets:', x_train.shape, x_train.shape, x_train.min(), x_train.max())\n",
    "\n",
    "x_train = x_train.reshape(60000, 28, 28, 1)\n",
    "x_test = x_test.reshape(10000, 28, 28, 1)\n",
    "\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0\n",
    "\n",
    "y_train = np.array(pd.get_dummies(y_train))\n",
    "y_test = np.array(pd.get_dummies(y_test))\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(filters=6, kernel_size=(5,5), input_shape=(28,28,1), padding='same', activation='sigmoid'))\n",
    "model.add(tf.keras.layers.AveragePooling2D(pool_size=(2,2)))\n",
    "model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(5,5), padding='valid', activation='sigmoid'))\n",
    "model.add(tf.keras.layers.AveragePooling2D(pool_size=(2,2)))\n",
    "model.add(tf.keras.layers.Conv2D(filters=120, kernel_size=(5,5), padding='valid', activation='sigmoid'))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(84, activation='sigmoid'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "model.build(input_shape=(None, 28, 28, 1))\n",
    "model.summary()\n",
    "\n",
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return x_train[idx], y_train[idx]\n",
    "\n",
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "                         for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics,\n",
    "          end=end)\n",
    "\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(x_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "\n",
    "# 补充一个关于accuracy的说明：\n",
    "# * 如果模型输出是一个数值，而标签也是一个数值，此时可以直接使用metrics=['accuracy']或者metrics= [keras.metrics.Accuracy()]\n",
    "# * 如果模型输出是一个多分类的概率列表，标签是一个onehot后的列表，则此时使用category_accuracy或者keras.metrics.CategoricalAccuracy()\n",
    "# * 如果模型输出是一个多分类的概率列表，标签是一个数值，则此时使用sparse_category_accuracy或者keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "metrics = [keras.metrics.MeanAbsoluteError(), keras.metrics.CategoricalAccuracy()]\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(x_train, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693992b2-8adc-435d-a2a9-08ad9ec9e224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c514e9ba-f830-49cb-9d69-79b918b31503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets: (60000, 28, 28) (60000,) 0 255\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 335,114\n",
      "Trainable params: 335,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "0 loss: 1.387618064880371 acc: 0.25\n",
      "200 loss: 0.42405617237091064 acc: 0.71140623\n",
      "400 loss: 0.3500107526779175 acc: 0.85796875\n",
      "600 loss: 0.31295889616012573 acc: 0.8645313\n",
      "800 loss: 0.23941634595394135 acc: 0.9021875\n",
      "1000 loss: 0.30272871255874634 acc: 0.89390624\n",
      "1200 loss: 0.2819403409957886 acc: 0.911875\n",
      "1400 loss: 0.20608016848564148 acc: 0.92046875\n",
      "1600 loss: 0.19578148424625397 acc: 0.91\n",
      "1800 loss: 0.19100677967071533 acc: 0.92875\n",
      "2000 loss: 0.17792671918869019 acc: 0.94703126\n",
      "2200 loss: 0.13450191915035248 acc: 0.93375\n",
      "2400 loss: 0.21769993007183075 acc: 0.93015623\n",
      "2600 loss: 0.17650413513183594 acc: 0.9415625\n",
      "2800 loss: 0.14067721366882324 acc: 0.9378125\n",
      "3000 loss: 0.21334664523601532 acc: 0.935\n",
      "3200 loss: 0.16363415122032166 acc: 0.9398438\n",
      "3400 loss: 0.12009237706661224 acc: 0.93671876\n",
      "3600 loss: 0.10962238907814026 acc: 0.9398438\n",
      "3800 loss: 0.17638109624385834 acc: 0.9578125\n",
      "4000 loss: 0.18811506032943726 acc: 0.95515627\n",
      "4200 loss: 0.1444128155708313 acc: 0.9409375\n",
      "4400 loss: 0.1398269534111023 acc: 0.9482812\n",
      "4600 loss: 0.18216700851917267 acc: 0.9490625\n",
      "4800 loss: 0.1509384661912918 acc: 0.9451563\n",
      "5000 loss: 0.14430004358291626 acc: 0.95375\n",
      "5200 loss: 0.2347138673067093 acc: 0.945625\n",
      "5400 loss: 0.2345765084028244 acc: 0.95234376\n",
      "5600 loss: 0.09152095764875412 acc: 0.9607813\n",
      "5800 loss: 0.14809143543243408 acc: 0.96203125\n",
      "6000 loss: 0.1153573989868164 acc: 0.95515627\n",
      "6200 loss: 0.16897344589233398 acc: 0.9507812\n",
      "6400 loss: 0.13930463790893555 acc: 0.9565625\n",
      "6600 loss: 0.12823805212974548 acc: 0.95109373\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-52521e18b3ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0macc_meter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ljh/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1072\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1075\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ljh/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ljh/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ljh/lib/python3.8/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1732\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1734\u001b[0;31m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1735\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ljh/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5694\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5695\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   5696\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5697\u001b[0m         transpose_b)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
    "\n",
    "\n",
    "(xs, ys),_ = datasets.mnist.load_data()\n",
    "print('datasets:', xs.shape, ys.shape, xs.min(), xs.max())\n",
    "\n",
    "xs = tf.convert_to_tensor(xs, dtype=tf.float32) / 255.\n",
    "db = tf.data.Dataset.from_tensor_slices((xs,ys))\n",
    "db = db.batch(32).repeat(10)\n",
    "\n",
    "optimizer = optimizers.SGD(lr=0.01)\n",
    "acc_meter = metrics.Accuracy()\n",
    "#criteon = losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "model = Sequential([layers.Dense(256, activation='relu'),\n",
    "                     layers.Dense(256, activation='relu'),\n",
    "                     layers.Dense(256, activation='relu'),\n",
    "                     layers.Dense(10)])\n",
    "model.build(input_shape=(None, 28*28))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "for step, (x,y) in enumerate(db):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # [b, 28, 28] => [b, 784]\n",
    "        x = tf.reshape(x, (-1, 28*28))\n",
    "        # [b, 784] => [b, 10]\n",
    "        out = model(x)\n",
    "        # [b] => [b, 10]\n",
    "        y_onehot = tf.one_hot(y, depth=10)\n",
    "        # [b, 10]\n",
    "        loss = tf.square(out-y_onehot)\n",
    "        # [b]\n",
    "        loss = tf.reduce_sum(loss) / 32\n",
    "\n",
    "\n",
    "        acc_meter.update_state(tf.argmax(out, axis=1), y)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "\n",
    "    if step % 200==0:\n",
    "\n",
    "        print(step, 'loss:', float(loss), 'acc:', acc_meter.result().numpy())\n",
    "        acc_meter.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a041dc1-6940-4b47-b214-d446038204c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
